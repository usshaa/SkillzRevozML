{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty to the model's complexity. Overfitting occurs when a model learns the noise in the training data rather than the underlying pattern, resulting in poor performance on unseen data. Regularization helps to create a simpler model that generalizes better to new data.\n",
    "\n",
    "Here's a step-by-step explanation of regularization with an example:\n",
    "\n",
    "### 1. Understanding Overfitting\n",
    "- **Overfitting**: A model performs very well on training data but poorly on test data.\n",
    "- **Underfitting**: A model performs poorly on both training and test data.\n",
    "\n",
    "### 2. Introduction to Regularization\n",
    "Regularization adds a penalty to the model's loss function to discourage it from becoming too complex. Two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "### 3. Linear Regression Example\n",
    "Let's consider a simple linear regression model:\n",
    "\\$ y = \\beta_0 + \\beta_1 x \\$\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the target variable.\n",
    "- \\( x \\) is the feature.\n",
    "- \\( \\beta_0 \\) and \\( \\beta_1 \\) are the coefficients to be learned.\n",
    "\n",
    "The loss function (Mean Squared Error) for this model is:\n",
    "\\$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\$\n",
    "\n",
    "### 4. Adding Regularization\n",
    "\n",
    "#### L2 Regularization (Ridge)\n",
    "L2 regularization adds a penalty equal to the sum of the squared coefficients:\n",
    "\\$ \\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\$\n",
    "\n",
    "Here, \\( \\lambda \\) is the regularization parameter that controls the strength of the penalty. As \\( \\lambda \\) increases, the penalty increases, leading to smaller coefficient values.\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "L1 regularization adds a penalty equal to the sum of the absolute values of the coefficients:\n",
    "\\$ \\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\$\n",
    "\n",
    "### 5. Example with Data\n",
    "Suppose we have the following data points for a simple linear regression problem:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 4 |\n",
    "| 3 | 6 |\n",
    "| 4 | 8 |\n",
    "| 5 | 10 |\n",
    "\n",
    "Without regularization, the model might fit perfectly:\n",
    "\\$ y = 2x \\$\n",
    "\n",
    "#### Adding L2 Regularization\n",
    "If we add L2 regularization with \\( \\lambda = 0.1 \\), the loss function becomes:\n",
    "\\$ \\text{Loss} = \\frac{1}{5} \\sum_{i=1}^{5} (y_i - (2x_i))^2 + 0.1 \\cdot (2^2) \\$\n",
    "\n",
    "This regularization term will shrink the coefficient \\( \\beta_1 \\), resulting in a model like:\n",
    "\\$ y = 1.9x \\$\n",
    "\n",
    "#### Adding L1 Regularization\n",
    "If we add L1 regularization with \\( \\lambda = 0.1 \\), the loss function becomes:\n",
    "\\$ \\text{Loss} = \\frac{1}{5} \\sum_{i=1}^{5} (y_i - (2x_i))^2 + 0.1 \\cdot |2| \\$\n",
    "\n",
    "This will also shrink the coefficient but may lead to some coefficients becoming zero, resulting in a model like:\n",
    "\\$ y = 1.8x \\$\n",
    "\n",
    "### 6. Choosing the Regularization Parameter\n",
    "The regularization parameter \\( \\lambda \\) is typically chosen using cross-validation. The goal is to find a balance where the model performs well on both training and validation data.\n",
    "\n",
    "### 7. Benefits of Regularization\n",
    "- **Reduces Overfitting**: Prevents the model from learning noise in the data.\n",
    "- **Simplifies the Model**: Encourages smaller or zero coefficients, leading to simpler models.\n",
    "- **Improves Generalization**: Enhances the model's performance on unseen data.\n",
    "\n",
    "### Summary\n",
    "Regularization is a crucial technique in machine learning to prevent overfitting and improve model generalization. By adding a penalty to the loss function, regularization encourages the model to remain simple, balancing the fit to training data and performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is essential in finding the optimal parameters for a model, which can significantly improve its performance. Two common methods for hyperparameter tuning are Grid Search and Random Search. Let's go through these methods with an example, using regularization for a linear regression model.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "#### 1. Generating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = pd.DataFrame(np.hstack((X, y)), columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097627</td>\n",
       "      <td>6.127731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.430379</td>\n",
       "      <td>9.191963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.205527</td>\n",
       "      <td>8.082243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.089766</td>\n",
       "      <td>5.733055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.847310</td>\n",
       "      <td>8.030181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.366383</td>\n",
       "      <td>5.780743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.173026</td>\n",
       "      <td>6.715668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.040215</td>\n",
       "      <td>3.431095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.657880</td>\n",
       "      <td>8.518108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.009391</td>\n",
       "      <td>4.045652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x         y\n",
       "0   1.097627  6.127731\n",
       "1   1.430379  9.191963\n",
       "2   1.205527  8.082243\n",
       "3   1.089766  5.733055\n",
       "4   0.847310  8.030181\n",
       "..       ...       ...\n",
       "95  0.366383  5.780743\n",
       "96  1.173026  6.715668\n",
       "97  0.040215  3.431095\n",
       "98  1.657880  8.518108\n",
       "99  0.009391  4.045652\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Splitting Data\n",
    "Split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Linear Regression with Regularization\n",
    "\n",
    "We will use Ridge (L2) and Lasso (L1) regression for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Hyperparameter Tuning\n",
    "\n",
    "##### Grid Search\n",
    "Grid Search exhaustively searches over a specified parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 50.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Ridge\n",
    "ridge_grid = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_ridge = ridge_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search for Lasso\n",
    "lasso_grid = GridSearchCV(lasso, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_lasso = lasso_grid.best_estimator_\n",
    "lasso_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search\n",
    "Random Search randomly samples the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search for Ridge\n",
    "ridge_random = RandomizedSearchCV(ridge, param_grid, n_iter=6, cv=5, scoring='neg_mean_squared_error', random_state=0)\n",
    "ridge_random.fit(X_train, y_train)\n",
    "best_ridge_random = ridge_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Search for Lasso\n",
    "lasso_random = RandomizedSearchCV(lasso, param_grid, n_iter=6, cv=5, scoring='neg_mean_squared_error', random_state=0)\n",
    "lasso_random.fit(X_train, y_train)\n",
    "best_lasso_random = lasso_random.best_estimator_\n",
    "lasso_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluating Models\n",
    "Evaluate the models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "ridge_pred = best_ridge.predict(X_test)\n",
    "lasso_pred = best_lasso.predict(X_test)\n",
    "ridge_random_pred = best_ridge_random.predict(X_test)\n",
    "lasso_random_pred = best_lasso_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0450193464317483, 1.125017634165578, 1.0450193464317483, 1.125017634165578)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error\n",
    "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "ridge_random_mse = mean_squared_error(y_test, ridge_random_pred)\n",
    "lasso_random_mse = mean_squared_error(y_test, lasso_random_pred)\n",
    "\n",
    "ridge_mse, lasso_mse, ridge_random_mse, lasso_random_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Choosing the Best Technique\n",
    "- **Grid Search**: Exhaustive and guarantees finding the best parameter combination within the specified grid, but can be computationally expensive for large grids.\n",
    "- **Random Search**: More efficient, especially when the parameter space is large, as it samples a fixed number of parameter combinations. It might not always find the optimal combination but often finds a good one in less time.\n",
    "\n",
    "**Comparison**:\n",
    "- **Grid Search** is more thorough and likely to find the optimal parameters if the grid is well-chosen.\n",
    "- **Random Search** is more efficient and faster, especially when dealing with a large number of hyperparameters.\n",
    "\n",
    "**Best Technique**:\n",
    "- For smaller parameter spaces, **Grid Search** is often preferred due to its thoroughness.\n",
    "- For larger parameter spaces or when computational resources are limited, **Random Search** is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
