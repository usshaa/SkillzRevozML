{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Techniques with Hyperparameter Tuning\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the performance of machine learning models. It helps in assessing how the results of a model will generalize to an independent dataset. Hyperparameter tuning involves finding the best set of hyperparameters for a machine learning algorithm. Combining cross-validation with hyperparameter tuning helps in selecting the optimal model parameters while minimizing overfitting.\n",
    "\n",
    "#### Common Cross-Validation Techniques\n",
    "\n",
    "1. **Holdout Validation:**\n",
    "   - Split the dataset into two parts: a training set and a testing set.\n",
    "   - Train the model on the training set and evaluate it on the testing set.\n",
    "   - Simple but can be unstable as it depends on a single split.\n",
    "\n",
    "2. **K-Fold Cross-Validation:**\n",
    "   - Split the dataset into K equally sized folds.\n",
    "   - For each fold, train the model on K-1 folds and validate it on the remaining fold.\n",
    "   - Repeat this process K times, each time using a different fold as the validation set.\n",
    "   - Average the performance across all K iterations.\n",
    "   - More reliable than holdout as it uses multiple splits.\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of optimizing the hyperparameters of a machine learning model to achieve the best performance. Common methods include:\n",
    "\n",
    "1. **Grid Search:**\n",
    "   - Define a grid of possible hyperparameter values.\n",
    "   - Evaluate the model for each combination of hyperparameters using cross-validation.\n",
    "   - Select the combination that yields the best performance.\n",
    "\n",
    "2. **Random Search:**\n",
    "   - Randomly sample hyperparameter values from the defined grid.\n",
    "   - Evaluate the model for each sampled combination using cross-validation.\n",
    "   - More efficient than grid search for large hyperparameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implementation\n",
    "\n",
    "Let's see how to combine cross-validation with hyperparameter tuning using scikit-learn.\n",
    "\n",
    "1. **Import Necessary Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Load Dataset:**\n",
    "\n",
    "For simplicity, we'll use a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 5)\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] + np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **K-Fold Cross-Validation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model: Ridge Regression\n",
    "model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE scores: [1.123026250858799, 1.310962679762743, 0.9738349081283368, 1.0792600611839485, 1.4403163291786139]\n",
      "Mean Cross-Validation MSE: 1.1854800458224883\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation scores\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_k, X_val_k = X_train[train_index], X_train[test_index]\n",
    "    y_train_k, y_val_k = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    model.fit(X_train_k, y_train_k)\n",
    "    y_pred_k = model.predict(X_val_k)\n",
    "    score = mean_squared_error(y_val_k, y_pred_k)\n",
    "    scores.append(score)\n",
    "\n",
    "print(f'Cross-Validation MSE scores: {scores}')\n",
    "print(f'Mean Cross-Validation MSE: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Hyperparameter Tuning with Grid Search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1,1.0, 10.0, 100.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_absolute_error') #'neg_mean_absolute_error','r2','neg_root_mean_squared_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Ridge(),\n",
       "             param_grid={'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1}\n",
      "Best cross-validation MSE: 0.8908434954079025\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters and corresponding score\n",
    "print(f'Best hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation MSE: {-grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.1460291479036144\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "print(f'Test MSE: {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Hyperparameter Tuning with Random Search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(Ridge(), param_dist, n_iter=10, cv=5, scoring='neg_mean_absolute_error', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:296: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=Ridge(),\n",
       "                   param_distributions={'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
       "                   random_state=0, scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.1}\n",
      "Best cross-validation MSE: 0.8908434954079025\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters and corresponding score\n",
    "print(f'Best hyperparameters: {random_search.best_params_}')\n",
    "print(f'Best cross-validation MSE: {-random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.1460291479036144\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model_random = random_search.best_estimator_\n",
    "y_pred_test_random = best_model_random.predict(X_test)\n",
    "test_mse_random = mean_squared_error(y_test, y_pred_test_random)\n",
    "print(f'Test MSE: {test_mse_random}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Combining cross-validation with hyperparameter tuning ensures that the model you select is both well-generalized and optimally configured. K-Fold cross-validation provides a robust estimate of model performance, while grid search and random search are effective techniques for finding the best hyperparameters. By using these methods, you can improve the accuracy and reliability of your machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
